{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb78147-7697-4672-8af3-8e789195da9f",
   "metadata": {},
   "source": [
    "# Training Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8510c36-6020-4e0a-aefd-e283791a9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import enum\n",
    "import copy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers, processors\n",
    "\n",
    "class Case(enum.Enum):\n",
    "  LOWER = 0\n",
    "  UPPER = 1\n",
    "  TITLE = 2\n",
    "\n",
    "class Whitespace(enum.Enum):\n",
    "  NONE = 0\n",
    "  SPACE = 1\n",
    "\n",
    "Path('tokenizers').mkdir(exist_ok=True)\n",
    "languages = ['arabic', 'azerbaijani', 'chinese', 'english', 'farsi', 'german', 'hebrew', 'hindi', 'korean', 'spanish', 'turkish', 'vietnamese']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee75ef9-151e-4dd1-ab7d-7e29e4a99cd5",
   "metadata": {},
   "source": [
    "Lengths\n",
    "```\n",
    "{'arabic': 1712361,\n",
    " 'azerbaijani': 1715809,\n",
    " 'chinese': 2879487,\n",
    " 'english': 2635469,\n",
    " 'farsi': 132568,\n",
    " 'german': 282059,\n",
    " 'hebrew': 20686,\n",
    " 'hindi': 40027,\n",
    " 'korean': 2632457,\n",
    " 'spanish': 4058317,\n",
    " 'turkish': 1810342,\n",
    " 'vietnamese': 2493325}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a45bed-707f-44ee-81ed-3d9f746da86c",
   "metadata": {},
   "source": [
    "## Vanilla Tokenizers\n",
    "\n",
    "Each token is a single integer. For benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed0054-3f02-4492-903e-bd56e29725ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "  dataset = load_dataset(\"Gabrui/multilingual_TinyStories\", lang)\n",
    "  \n",
    "  special_toks = {'pad_token':\"<|PAD|>\", 'bos_token':\"<|BOS|>\", 'eos_token':\"<|EOS|>\", 'unk_token':\"<|UNK|>\"}\n",
    "  def batch_iterator(batch_size=1000):\n",
    "      tok_dataset = dataset['train'].select_columns(\"story\")\n",
    "      for batch in tok_dataset.iter(batch_size):\n",
    "          yield batch[\"story\"]\n",
    "  \n",
    "  tokenizer = Tokenizer(models.BPE(unk_token='<|UNK|>'))\n",
    "  tokenizer.normalizer = normalizers.NFKC()\n",
    "  tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.UnicodeScripts(),\n",
    "                                                     pre_tokenizers.Digits(individual_digits=True), pre_tokenizers.Metaspace(prepend_scheme='never')])\n",
    "  tokenizer.add_special_tokens(list(special_toks.values()))\n",
    "  tokenizer.post_processor = processors.TemplateProcessing(\n",
    "      single=\"<|BOS|> $A <|EOS|>\",\n",
    "      pair=\"<|BOS|> $A <|EOS|> <|BOS|>:1 $B:1 <|EOS|>:1\",\n",
    "      special_tokens=[\n",
    "          (\"<|BOS|>\", tokenizer.token_to_id(\"<|BOS|>\")),\n",
    "          (\"<|EOS|>\", tokenizer.token_to_id(\"<|EOS|>\")),\n",
    "      ],\n",
    "  )\n",
    "  tokenizer.decoder = decoders.Metaspace(prepend_scheme='never')\n",
    "  if not Path(f'vanilla_{lang}.json').exists() or (RETRAIN_TOK:=False):\n",
    "    trainer = trainers.BpeTrainer(vocab_size=15000, special_tokens=list(special_toks.values()), show_progress=True) # min_frequency=20,\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer, length=len(dataset['train']))\n",
    "    # tokenizer.save(f'vanilla_{lang}.json')\n",
    "  else:\n",
    "    tokenizer = Tokenizer.from_file(f'vanilla_{lang}.json')\n",
    "  \n",
    "  fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "  fast_tokenizer.add_special_tokens(special_toks)\n",
    "  fast_tokenizer.save_pretrained(f'tokenizers/{lang}_vanilla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebfdb0-9b27-45c4-acaa-9189f612b024",
   "metadata": {},
   "source": [
    "## Multidimensional Tokenizers\n",
    "\n",
    "Each token is mapped to a tuple of integers. Currently we model only if the token starts with a space and what kind of case it has (lower case, upper case, or title case), resulting in the tuple: `(token_id, has_whitespace, case)`.\n",
    "- `token_id`: the id of the originally trainned token (lower case, without space);\n",
    "- `has_whitespace`: true (`1`) if the token starts with a space;\n",
    "- `case`: indicates if the token is lower case, upper case, or title case.\n",
    "\n",
    "For a simpler implementation, the tokenizers library is used training with 'stemmed' (lower case, without space) data and manually augmenting the trained tokenizer with the extended vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc0c0a-998c-474f-950a-4b8cad4dff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "  print(lang)\n",
    "  dataset = load_dataset(\"Gabrui/multilingual_TinyStories\", lang)\n",
    "  \n",
    "  special_tokens = [\"<|PAD|>\", \"<|BOS|>\", \"<|EOS|>\", \"<|UNK|>\"]\n",
    "  special_toks = {'pad_token':\"<|PAD|>\", 'bos_token':\"<|BOS|>\", 'eos_token':\"<|EOS|>\", 'unk_token':\"<|UNK|>\"}\n",
    "  def batch_iterator(batch_size=1000):\n",
    "      tok_dataset = dataset['train'].select_columns(\"story\")\n",
    "      for batch in tok_dataset.iter(batch_size):\n",
    "          yield batch[\"story\"]\n",
    "  \n",
    "  \n",
    "  tokenizer = Tokenizer(models.BPE(unk_token='<|UNK|>'))\n",
    "  tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase(), normalizers.NFKC()])\n",
    "  tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.BertPreTokenizer(), pre_tokenizers.UnicodeScripts(),\n",
    "                                                     pre_tokenizers.Digits(individual_digits=True), pre_tokenizers.Metaspace(prepend_scheme='never')])\n",
    "  tokenizer.decoder = decoders.Metaspace(prepend_scheme='never')\n",
    "  tokenizer.add_special_tokens(list(special_toks.values()))\n",
    "  trainer = trainers.BpeTrainer(vocab_size=15000, special_tokens=special_tokens, show_progress=True)\n",
    "  tokenizer.train_from_iterator(batch_iterator(), trainer, length=len(dataset['train']))\n",
    "  \n",
    "  # For inference it should be lossless\n",
    "  tokenizer.normalizer = normalizers.NFKC()\n",
    "  tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(prepend_scheme='never')\n",
    "  tokenizer.post_processor = processors.TemplateProcessing(\n",
    "      single=\"<|BOS|> $A <|EOS|>\",\n",
    "      pair=\"<|BOS|> $A <|EOS|> <|BOS|>:1 $B:1 <|EOS|>:1\",\n",
    "      special_tokens=[\n",
    "          (\"<|BOS|>\", tokenizer.token_to_id(\"<|BOS|>\")),\n",
    "          (\"<|EOS|>\", tokenizer.token_to_id(\"<|EOS|>\")),\n",
    "      ],\n",
    "  )\n",
    "  tokenizer_data = json.loads(tokenizer.to_str())\n",
    "  vocab_stem = tokenizer_data['model']['vocab']\n",
    "  merge_stem = tokenizer_data['model']['merges']\n",
    "  \n",
    "  items = list(vocab_stem.items())\n",
    "  vocab = {k: v for k, v in items[:len(special_tokens)]}\n",
    "  id2multi = {v: (v, Whitespace.NONE, Case.LOWER) for _, v in items[:len(special_tokens)]}\n",
    "  merge = []\n",
    "  strange = []\n",
    "  cid = len(special_tokens)\n",
    "  n0 = len(special_tokens)\n",
    "  assert cid == items[n0][-1]\n",
    "  \n",
    "  # Initializing vocab and checking charset\n",
    "  for tok, tid in items[n0:]:\n",
    "    if len(tok) > 1:\n",
    "      break\n",
    "    vocab[tok] = cid\n",
    "    id2multi[cid] = (tid, Whitespace.NONE, Case.LOWER)\n",
    "    cid += 1\n",
    "    # Adding uppercase for single characters\n",
    "    if tok != tok.upper() and vocab.get(tok.upper()) is None:\n",
    "      if len(tok.upper()) > 1:\n",
    "        strange.append((tok, cid, tok.upper()))\n",
    "        continue\n",
    "      vocab[tok.upper()] = cid\n",
    "      id2multi[cid] = (tid, Whitespace.NONE, Case.UPPER)\n",
    "      cid += 1\n",
    "  \n",
    "  # Adding whitespaces characters to vocab\n",
    "  t0 = tid + 1\n",
    "  re_add = ['▁', '\\t', '\\n']\n",
    "  for i, tok in enumerate(re_add):\n",
    "    vocab[tok] = cid\n",
    "    id2multi[cid] = (t0+i, Whitespace.NONE, Case.LOWER)\n",
    "    cid += 1\n",
    "  n_re = len(re_add)\n",
    "  \n",
    "  # Adding merge with space\n",
    "  for tok, nid in list(vocab.items())[n0:-n_re]:\n",
    "    merge.append(['▁', tok])\n",
    "    vocab[f'▁{tok}'] = cid\n",
    "    id2multi[cid] = (id2multi[nid][0], Whitespace.SPACE, id2multi[nid][-1])\n",
    "    cid += 1\n",
    "  \n",
    "  # Adding the rest, all combinations if not repeated\n",
    "  assert len(merge_stem) == len(vocab_stem) - t0 + 1\n",
    "  for i, (tok, tid) in enumerate(items[t0-1:]):\n",
    "    for case in Case:\n",
    "      for wspc in Whitespace:\n",
    "        rtok = tok\n",
    "        r0, r1 = merge_stem[i]\n",
    "        if case is Case.UPPER:\n",
    "          rtok = tok.upper()\n",
    "          r0, r1 = r0.upper(), r1.upper()\n",
    "        elif case is Case.TITLE:\n",
    "          rtok = tok.title()\n",
    "          r0 = r0.title()\n",
    "        if rtok == tok and case is not Case.LOWER:\n",
    "          continue\n",
    "        if wspc is Whitespace.SPACE:\n",
    "          rtok = f'▁{rtok}'\n",
    "          r0 = f'▁{r0}'\n",
    "        if vocab.get(rtok) is not None:\n",
    "          continue\n",
    "        merge.append([r0, r1])\n",
    "        vocab[rtok] = cid\n",
    "        id2multi[cid] = (tid+n_re, wspc, case)\n",
    "        cid += 1\n",
    "  \n",
    "  # Verify if everything is all right\n",
    "  i = 0\n",
    "  for (tok, cid), (cid2, tupl) in zip(vocab.items(), id2multi.items()):\n",
    "    assert cid == i\n",
    "    i += 1\n",
    "    assert cid == cid2\n",
    "    tid, wspc, case = tupl\n",
    "    if tid < n0:\n",
    "      continue\n",
    "    if wspc is Whitespace.SPACE:\n",
    "      assert tok[0] == '▁'\n",
    "    if case is Case.LOWER:\n",
    "      assert tok.lower() == tok\n",
    "    elif case is Case.UPPER:\n",
    "      assert tok.upper() == tok\n",
    "      assert tok.lower() != tok\n",
    "    else:\n",
    "      assert tok.title() == tok\n",
    "      assert tok.lower() != tok\n",
    "    tok_orig = (tok[1:] if wspc is Whitespace.SPACE else tok).lower()\n",
    "    if tid < t0:\n",
    "      assert vocab_stem[tok_orig] == tid\n",
    "    elif tid >= t0 + n_re:\n",
    "      if vocab_stem.get(tok_orig) is None:\n",
    "        tok_orig_real = items[tid - n_re][0]\n",
    "        assert tok_orig.upper().lower() == tok_orig_real.lower().upper().lower()\n",
    "        tok_orig = tok_orig_real\n",
    "      assert vocab_stem[tok_orig] == tid - n_re or tok_orig.upper().lower() == items[tid - n_re][0].upper().lower()\n",
    "  for i, mer in enumerate(merge):\n",
    "    assert vocab[''.join(mer)] > i\n",
    "  for orig, i, upper in strange:\n",
    "    assert vocab[upper]\n",
    "  \n",
    "  id2multi_arr = np.array([(v[0], v[1].value, v[2].value) for k, v in id2multi.items()])\n",
    "  tokenizer_data_extended = copy.deepcopy(tokenizer_data)\n",
    "  tokenizer_data_extended['model']['vocab'] = vocab\n",
    "  tokenizer_data_extended['model']['merges'] = merge\n",
    "  tokenizer_extended = Tokenizer.from_str(json.dumps(tokenizer_data_extended))\n",
    "  fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer_extended)\n",
    "  fast_tokenizer.add_special_tokens(special_toks)\n",
    "  fast_tokenizer.save_pretrained(f'tokenizers/{lang}_multi')\n",
    "  np.savetxt(f'tokenizers/{lang}_multi/multi.txt', id2multi_arr, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
